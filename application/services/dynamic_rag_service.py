# application/services/dynamic_rag_service.py
from typing import List, Dict, Any, Optional
from datetime import datetime
import json
import logging
from domain.entities.chat_message import ChatMessage, MessageRole
from domain.utils.result import Result
from application.services.chat_agent_service import ChatAgentService
from application.services.json_embedding_service import JSONEmbeddingService

class DynamicRAGService:
    """Serwis do dynamicznych zapytaÅ„ RAG decydowanych przez LLM (inspirowany ChatElioraReflect)"""
    
    def __init__(self, llm_service=None, knowledge_service=None, conversation_service=None, embedding_service=None):
        self.llm_service = llm_service
        self.knowledge_service = knowledge_service
        self.conversation_service = conversation_service
        self.json_embedding_service = JSONEmbeddingService(embedding_service=embedding_service)
        self.logger = logging.getLogger(__name__)
    
    async def decide_vector_query(
        self, 
        conversation_context: List[ChatMessage], 
        current_message: str,
        user_context: Optional[dict] = None
    ) -> Result[str, str]:
        """Decyduje jakie zapytanie zadaÄ‡ do bazy wektorowej (jak GetStreamHistoryFromDb w ChatElioraReflect)"""
        try:
            # Buduje prompt analizy (jak w ChatElioraReflect)
            analysis_prompt = self._build_rag_analysis_prompt(
                conversation_context, 
                current_message, 
                user_context
            )
            
            # WysyÅ‚a do LLM do analizy
            analysis_result = await self._analyze_for_rag_query(analysis_prompt)
            
            if analysis_result.is_error:
                return Result.error(f"Analiza RAG nie powiodÅ‚a siÄ™: {analysis_result.error}")
            
            analysis = analysis_result.value
            vector_query = analysis.get("vector_query", "")
            
            if not vector_query:
                return Result.error("Nie wygenerowano zapytania wektorowego")
            
            self.logger.info(f"ğŸ§  LLM zdecydowaÅ‚ o zapytaniu wektorowym: {vector_query}")
            return Result.success(vector_query)
            
        except Exception as e:
            return Result.error(f"Decyzja o dynamicznym zapytaniu RAG nie powiodÅ‚a siÄ™: {str(e)}")
    
    def _build_rag_analysis_prompt(
        self, 
        conversation_context: List[ChatMessage], 
        current_message: str,
        user_context: Optional[dict] = None
    ) -> str:
        """Buduje prompt analizy dla decyzji o zapytaniu RAG"""
        
        # Buduje historiÄ™ rozmowy
        conversation_text = ""
        for msg in conversation_context[-4:]:  # Ostatnie 4 wiadomoÅ›ci
            role = "User" if msg.role == MessageRole.USER else "Assistant"
            conversation_text += f"{role}: {msg.content}\n"
        
        # TODO: DodaÄ‡ kontekst uÅ¼ytkownika gdy system logowania zostanie zaimplementowany
        user_context_info = ""
        if user_context:
            user_context_info = f"\nKONTEKST UÅ»YTKOWNIKA:\nRola: {user_context.get('role', 'user')}\nUprawnienia: {user_context.get('permissions', [])}"
        
        analysis_prompt = f"""JesteÅ› agentem analizy rozmÃ³w z refleksyjnymi zdolnoÅ›ciami meta-myÅ›lenia.

KONTEKST ROZMOWY:
{conversation_text}

OBECNA WIADOMOÅšÄ† UÅ»YTKOWNIKA:
{current_message}
{user_context_info}

ZADANIE ANALIZY:
Przeanalizuj ten kontekst rozmowy i zdecyduj jakie zapytanie zadaÄ‡ do bazy wektorowej.
1. Jaki jest gÅ‚Ã³wny temat/przedmiot dyskusji?
2. Jakie konkretne informacje mogÄ… byÄ‡ potrzebne z bazy wiedzy?
3. Jakie byÅ‚oby najbardziej efektywne zapytanie do wyszukania odpowiednich informacji?

Odpowiedz w formacie JSON:
{{
    "main_topic": "string",
    "information_needed": "string", 
    "vector_query": "string",
    "reasoning": "string"
}}"""
        
        return analysis_prompt
    
    async def _analyze_for_rag_query(self, analysis_prompt: str) -> Result[Dict[str, Any], str]:
        """Analizuje rozmowÄ™ uÅ¼ywajÄ…c LLM do decyzji o zapytaniu RAG"""
        try:
            # UÅ¼ywa LLM Service z ChatAgentService (z Container)
            self.logger.info(f"ğŸ” DynamicRAG: WysyÅ‚anie promptu analizy do LLM...")
            
            # Buduje wiadomoÅ›ci dla LLM
            from datetime import datetime
            analysis_messages = [
                ChatMessage(role=MessageRole.SYSTEM, content="JesteÅ› ekspertem w analizie rozmÃ³w i generowaniu zapytaÅ„ do bazy wektorowej.", timestamp=datetime.now()),
                ChatMessage(role=MessageRole.USER, content=analysis_prompt, timestamp=datetime.now())
            ]
            
            # WywoÅ‚uje LLM Service bezpoÅ›rednio (z Container)
            result = await self.llm_service.get_completion(analysis_messages)
            
            if result.is_success:
                response_value = result.value
                self.logger.info(f"ğŸ” DynamicRAG: Otrzymano odpowiedÅº od LLM: {type(response_value)} - {str(response_value)[:200]}...")
                
                # LLM Service zwraca bezpoÅ›rednio string
                response = str(response_value)
                
                # NOWA METODA: UÅ¼yj JSONEmbeddingService do parsowania
                self.logger.info(f"ğŸ“ DynamicRAG: UÅ¼ywanie JSONEmbeddingService do parsowania...")
                processing_result = await self.json_embedding_service.process_json_for_rag(
                    response,
                    query_key="vector_query",
                    auto_create_embedding=True
                )
                
                if processing_result.is_success:
                    processed_data = processing_result.value
                    parsed_json = processed_data["parsed_json"]
                    self.logger.info(f"âœ… DynamicRAG: Successfully parsed JSON with {len(parsed_json)} keys")
                    
                    # SprawdÅº czy mamy embedding
                    if "embedding" in processed_data:
                        self.logger.info(f"ğŸ“Š DynamicRAG: Utworzono embedding o wymiarach: {processed_data['embedding_dim']}")
                    
                    return Result.success(parsed_json)
                else:
                    self.logger.warning(f"âš ï¸ DynamicRAG: JSON parsing failed, fallback to text extraction: {processing_result.error}")
                    # Fallback: wyciÄ…ga informacje z odpowiedzi tekstowej
                    analysis = self._extract_rag_analysis_from_text(response)
                    return Result.success(analysis)
            else:
                return Result.error(f"Analiza LLM nie powiodÅ‚a siÄ™: {result.error}")
                
        except Exception as e:
            return Result.error(f"BÅ‚Ä…d analizy RAG: {str(e)}")
    
    def _extract_rag_analysis_from_text(self, text: str) -> Dict[str, Any]:
        """WyciÄ…ga analizÄ™ RAG z odpowiedzi tekstowej gdy parsowanie JSON nie powiedzie siÄ™"""
        return {
            "main_topic": "analiza_rozmowy",
            "information_needed": "informacje_kontekstowe",
            "vector_query": text[:100] + "...",  # UÅ¼ywa pierwszych 100 znakÃ³w
            "reasoning": "WyciÄ…gniÄ™te z odpowiedzi tekstowej",
            "raw_response": text
        }
    
    async def search_with_filtering(
        self, 
        query: str, 
        score_threshold: float = 0.85,
        limit: int = 20,
        user_context: Optional[dict] = None
    ) -> Result[List[Dict[str, Any]], str]:
        """Wyszukuje w bazie wektorowej z filtrowaniem (jak GetValueFromVDB w ChatElioraReflect)"""
        try:
            # Wykonuje wyszukiwanie wektorowe przez KnowledgeService
            search_result = await self.knowledge_service.search_knowledge_base(query, limit=limit)
            
            if search_result.is_error:
                return Result.error(f"Wyszukiwanie wektorowe nie powiodÅ‚o siÄ™: {search_result.error}")
            
            vector_results = search_result.value
            
            # Filtruje wyniki wedÅ‚ug score (jak w ChatElioraReflect)
            filtered_results = []
            for result in vector_results:
                if isinstance(result, dict):
                    score = result.get('score', 0.0)
                    if score >= score_threshold:
                        filtered_results.append(result)
                else:
                    # JeÅ›li brak informacji o score, uwzglÄ™dnia wszystkie wyniki
                    filtered_results.append(result)
            
            self.logger.info(f"ğŸ” Znaleziono {len(filtered_results)} wynikÃ³w (przefiltrowane z {len(vector_results)})")
            
            # TODO: DodaÄ‡ filtrowanie specyficzne dla uÅ¼ytkownika gdy system logowania zostanie zaimplementowany
            if user_context and user_context.get('role') != 'admin':
                # UÅ¼ytkownicy nie-admin otrzymujÄ… ograniczone wyniki
                filtered_results = filtered_results[:10]
            
            return Result.success(filtered_results)
            
        except Exception as e:
            return Result.error(f"Przefiltrowane wyszukiwanie wektorowe nie powiodÅ‚o siÄ™: {str(e)}")
