import logging
from typing import List, Optional, AsyncGenerator
from datetime import datetime
from domain.entities.chat_message import ChatMessage, MessageRole
from domain.utils.result import Result
from domain.services.SystemPromptsService import SystemPromptsService

class PromptService:
    """Serwis do budowania kompletnych list wiadomoÅ›ci z promptami systemowymi (inspirowany ChatElioraReflect)"""
    
    def __init__(self, knowledge_service=None, system_prompts_service: Optional[SystemPromptsService] = None):
        self.knowledge_service = knowledge_service
        self.system_prompts = system_prompts_service or SystemPromptsService()
        self.logger = logging.getLogger(__name__)
    
    def get_global_system_prompts(self, user_context: Optional[dict] = None) -> List[ChatMessage]:
        """Pobiera globalne prompty systemowe (jak GetGlobalChatMessages w ChatElioraReflect)"""
        prompts = [
            # 1. OSOBOWOÅšÄ† ELIORA - jeden prompt
            ChatMessage(
                role=MessageRole.SYSTEM,
                content=self.system_prompts.eliora_personality_prompt,
                timestamp=datetime.now()
            ),
            # 2. KOLORY - jeden prompt
            ChatMessage(
                role=MessageRole.SYSTEM,
                content=self.system_prompts.color_syntax_prompt,
                timestamp=datetime.now()
            )
        ]
        
        # TODO: DodaÄ‡ prompty specyficzne dla uÅ¼ytkownika gdy system logowania zostanie zaimplementowany
        if user_context:
            prompts.append(ChatMessage(
                role=MessageRole.SYSTEM,
                content=self.system_prompts.get_user_profile_prompt(user_context),
                timestamp=datetime.now()
            ))
        
        return prompts
    
    def get_additional_system_prompts(self, user_context: Optional[dict] = None) -> List[ChatMessage]:
        """Pobiera dodatkowe prompty systemowe (jak GetAdditionalChatMessage w ChatElioraReflect)"""
        prompts = [
            ChatMessage(
                role=MessageRole.SYSTEM,
                content=self.system_prompts.general_conversation_role,
                timestamp=datetime.now()
            )
        ]
        
        # TODO: DodaÄ‡ prompty specyficzne dla roli gdy system logowania zostanie zaimplementowany
        if user_context and user_context.get('role') == 'admin':
            prompts.append(ChatMessage(
                role=MessageRole.SYSTEM,
                content=self.system_prompts.admin_role_prompt,
                timestamp=datetime.now()
            ))
        
        return prompts
    
    def build_complete_message_list(
        self, 
        user_message: str, 
        idioms: List[str], 
        conversation_history: List[ChatMessage],
        user_context: Optional[dict] = None
    ) -> List[ChatMessage]:
        """Buduje kompletnÄ… listÄ™ wiadomoÅ›ci (jak GetPromptWindowList w ChatElioraReflect)"""
        self.logger.info(f"ğŸ­ PromptService: Budowanie listy wiadomoÅ›ci - user_message: '{user_message}', idioms: {len(idioms)}, history: {len(conversation_history)}")
        messages = []
        
        # 1. Globalne prompty systemowe
        messages.extend(self.get_global_system_prompts(user_context))
        
        # 2. Dodatkowe prompty systemowe
        messages.extend(self.get_additional_system_prompts(user_context))
        
        # 3. WSZYSTKIE IDIOMY RAZEM - jeden system prompt (jak SetBaseIdioms)
        if idioms and any(idiom.strip() for idiom in idioms if idiom):
            # ÅÄ…czy wszystkie idiomy w jeden prompt
            combined_idioms = "\n\n".join([idiom.strip() for idiom in idioms if idiom and idiom.strip()])
            messages.append(ChatMessage(
                role=MessageRole.SYSTEM,
                content=self.system_prompts.get_idioms_prompt(combined_idioms),
                timestamp=datetime.now()
            ))
            self.logger.info(f"ğŸ­ PromptService: Dodano {len([i for i in idioms if i and i.strip()])} idiomÃ³w w jednym system prompt")
        else:
            self.logger.warning(f"âš ï¸ PromptService: Brak idiomÃ³w do dodania")
        
        # 4. Historia rozmowy (ostatnie 3 wiadomoÅ›ci)
        messages.extend(conversation_history[-3:])
        
        # 5. Aktualna wiadomoÅ›Ä‡ uÅ¼ytkownika
        if user_message and user_message.strip():
            messages.append(ChatMessage(
                role=MessageRole.USER,
                content=user_message,
                timestamp=datetime.now()
            ))
            self.logger.info(f"ğŸ­ PromptService: Dodano wiadomoÅ›Ä‡ uÅ¼ytkownika: '{user_message}'")
        else:
            self.logger.error(f"âŒ PromptService: Pusta wiadomoÅ›Ä‡ uÅ¼ytkownika!")
            raise ValueError("User message cannot be empty")
        
        self.logger.info(f"ğŸ­ PromptService: Zbudowano {len(messages)} wiadomoÅ›ci")
        return messages
    
    async def send_to_llm_streaming(
        self, 
        messages: List[ChatMessage], 
        llm_service
    ) -> AsyncGenerator[str, None]:
        """WysyÅ‚a do LLM z streamingiem (jak SendStreamToLLM w ChatElioraReflect)"""
        self.logger.info(f"ğŸ­ PromptService: WysyÅ‚anie {len(messages)} wiadomoÅ›ci do LLM")
        
        # Debug: sprawdÅº czy messages nie sÄ… puste
        for i, msg in enumerate(messages):
            self.logger.info(f"ğŸ­ WiadomoÅ›Ä‡ {i+1}: {msg.role.value} - {msg.content[:100]}...")
        
        async for chunk in llm_service.stream_completion(messages):
            if chunk.is_success:
                if chunk.value:  # SprawdÅº czy chunk nie jest pusty
                    yield chunk.value
                else:
                    self.logger.warning("âš ï¸ PromptService: Otrzymano pusty chunk od LLM")
            else:
                self.logger.error(f"âŒ BÅ‚Ä…d streamingu LLM: {chunk.error}")
                break
