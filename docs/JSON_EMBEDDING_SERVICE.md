# JSONEmbeddingService - Obs≈Çuga JSON i Embedding√≥w

## üìã PrzeglƒÖd

`JSONEmbeddingService` obs≈Çuguje parsowanie pierwszego JSON zwr√≥conego przez model oraz automatyczne generowanie embeddingu dla wyszukiwania w bazie wektorowej.

## üéØ Problem, kt√≥ry rozwiƒÖzuje

### Przed
```python
# Hardcoded parsowanie JSON
response = str(result.value)
try:
    analysis = json.loads(response)
except json.JSONDecodeError:
    # Fallback...
```

### Po
```python
# Centralny serwis z inteligentnym parsowaniem
result = await json_embedding_service.process_json_for_rag(
    llm_response,
    query_key="vector_query",
    auto_create_embedding=True
)
```

## üîß Funkcjonalno≈õƒá

### 1. Inteligentne parsowanie JSON

Obs≈Çuguje wiele format√≥w odpowiedzi z modelu:

```python
# Format 1: Bezpo≈õredni JSON
{"vector_query": "..."}

# Format 2: JSON w bloku code
```json
{"vector_query": "..."}
```

# Format 3: Python dict syntax
{"vector_query": True, "reasoning": None}

# Format 4: Text z ukrytym dict
Some text before {dict} more text
```

### 2. Ekstrakcja zapytania wektorowego

```python
# Automatycznie znajdzie pole "vector_query" w JSON
result = await json_embedding_service.extract_and_embed_from_json(
    llm_response,
    query_key="vector_query"
)

# parsed_json: {"vector_query": "...", "reasoning": "..."}
# query_text: "..."
```

### 3. Automatyczne generowanie embeddingu

```python
# Jednym wywo≈Çaniem: parse + extract + embed
result = await json_embedding_service.process_json_for_rag(
    llm_response,
    auto_create_embedding=True
)

# Zwraca:
# {
#   "parsed_json": {...},
#   "query_text": "...",
#   "embedding": [0.1, 0.2, ...],
#   "embedding_dim": 1024
# }
```

## üìä Integracja z DynamicRAG

```python
class DynamicRAGService:
    def __init__(self, ..., embedding_service=None):
        self.json_embedding_service = JSONEmbeddingService(
            embedding_service=embedding_service
        )
    
    async def _analyze_for_rag_query(self, analysis_prompt: str):
        # Wywo≈Çaj LLM
        result = await self.llm_service.get_completion(analysis_messages)
        response = str(result.value)
        
        # U≈ºyj JSONEmbeddingService
        processing = await self.json_embedding_service.process_json_for_rag(
            response,
            query_key="vector_query",
            auto_create_embedding=True
        )
        
        if processing.is_success:
            # Masz JSON i embedding gotowy do u≈ºycia!
            parsed_json = processing.value["parsed_json"]
            embedding = processing.value.get("embedding")
            
            # Teraz mo≈ºesz wyszukaƒá w bazie wektorowej u≈ºywajƒÖc embedding
            search_results = await self.knowledge_service.search_by_embedding(
                embedding
            )
```

## üîç Metody API

### `extract_and_embed_from_json(llm_response, query_key)`

Ekstraktuje JSON i wyciƒÖga okre≈õlone pole.

```python
result = await service.extract_and_embed_from_json(
    '{"vector_query": "AI ethics", "reasoning": "..."}',
    query_key="vector_query"
)

# Returns: ({"vector_query": "...", "reasoning": "..."}, "AI ethics")
```

### `create_embedding_for_query(query_text)`

Tworzy embedding dla tekstu.

```python
result = await service.create_embedding_for_query("AI ethics")
# Returns: [0.1, 0.2, 0.3, ...]  # embedding vector
```

### `process_json_for_rag(llm_response, query_key, auto_create_embedding)`

Kompletne przetworzenie: parse + extract + embed.

```python
result = await service.process_json_for_rag(
    llm_response="<JSON response>",
    query_key="vector_query",
    auto_create_embedding=True
)

# Returns: {
#   "parsed_json": {...},
#   "query_text": "...",
#   "embedding": [...],
#   "embedding_dim": 1024
# }
```

## üéì Przyk≈Çad u≈ºycia

### Scenariusz: Dynamiczne zapytania RAG

```python
# 1. Model zwraca JSON z analizƒÖ
llm_response = """
{
    "main_topic": "AI ethics",
    "information_needed": "Philosophical frameworks",
    "vector_query": "What are philosophical frameworks for AI ethics?",
    "reasoning": "User is asking about ethical frameworks"
}
```

# 2. JSONEmbeddingService parsuje i wyciƒÖga pole "vector_query"
result = await json_embedding_service.process_json_for_rag(
    llm_response,
    query_key="vector_query"
)

# 3. Otrzymujesz embedding gotowy do u≈ºycia
if result.is_success:
    embedding = result.value["embedding"]
    query_text = result.value["query_text"]
    
    # 4. Wyszukaj w bazie wektorowej u≈ºywajƒÖc embedding
    search_results = await vector_db.search_by_embedding(
        embedding,
        limit=5
    )
```

## üîÑ Przep≈Çyw danych

```
LLM Response (JSON string)
  ‚Üì
extract_json_from_response()
  ‚Üì
Parsed JSON dict
  ‚Üì
extract_query_text(query_key)
  ‚Üì
Query Text
  ‚Üì
create_embedding_for_query()
  ‚Üì
Embedding Vector [0.1, 0.2, ...]
  ‚Üì
Vector Database Search
```

## üìù Walidacja

```python
# Sprawd≈∫ czy JSON ma poprawnƒÖ strukturƒô
is_valid = service.validate_json_structure(parsed_json)

# Walidacja sprawdza:
# - Czy to jest dict
# - Czy ma klucze
# - Czy struktura jest ok
```

## üõ†Ô∏è Konfiguracja

### Embedding Service

Serwis wymaga przekazania `embedding_service`:

```python
from infrastructure.ai.embedding.lmstudio_embedding_service import LMStudioEmbeddingService

embedding_service = LMStudioEmbeddingService(...)
json_embedding_service = JSONEmbeddingService(
    embedding_service=embedding_service
)
```

### Auto-fallback

Je≈õli embedding service nie jest dostƒôpny, zwraca dummy vector:

```python
# Bez embedding service
result = await service.create_embedding_for_query("test")
# Returns: [0.1] * 1024  # Dummy vector
```

## üìç Lokalizacja

```
application/services/json_embedding_service.py
```

## üîó PowiƒÖzane serwisy

- `DynamicRAGService` - u≈ºywa JSONEmbeddingService
- `KnowledgeService` - wyszukiwanie w bazie wektorowej
- `EmbeddingService` - generowanie embedding√≥w

## üí° Zalety

1. **Inteligentne parsowanie** - obs≈Çuguje wiele format√≥w JSON
2. **Automatyczne embeddingi** - tworzy embedding u≈ºywajƒÖc w≈Ça≈õciwego query
3. **Fallback handling** - graceful degradation gdy co≈õ siƒô nie udaje
4. **Centralizacja** - jedna lokalizacja dla parsowania JSON
5. **Logging** - szczeg√≥≈Çowe logi dla debugowania

## üöÄ Przysz≈Çe rozszerzenia

- [ ] Cache dla embedding√≥w
- [ ] Validacja schematu JSON (JSON Schema)
- [ ] Obs≈Çuga wielu zapyta≈Ñ (batch embeddings)
- [ ] Metryki jako≈õci embeddings
- [ ] Konfigurowalne strategie parsowania

