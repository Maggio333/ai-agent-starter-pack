# ğŸ¯ Voice AI Assistant

Zaawansowany system chatbot z architekturÄ… Clean Architecture, integrujÄ…cy Flutter frontend, FastAPI backend, bazÄ™ wektorowÄ… i inteligentnÄ… analizÄ™ kontekstu.

## ğŸš€ Quick Start

### Opcja 1: Docker (Zalecane) ğŸ³

Najprostszy sposÃ³b uruchomienia caÅ‚ego systemu:

```bash
cd python_agent

# 1. Skopiuj plik konfiguracyjny
cp env.example .env

# 2. Uruchom LM Studio na hoÅ›cie (port 8123)
#    - Zainstaluj z https://lmstudio.ai/
#    - ZaÅ‚aduj model
#    - Uruchom Local Server (Settings â†’ Local Server)

# 3. Uruchom wszystkie serwisy
docker-compose up --build

# 4. DostÄ™p:
#    - Frontend: http://localhost:3000
#    - Backend API: http://localhost:8080
#    - Qdrant UI: http://localhost:6333/dashboard
```

**WiÄ™cej informacji**: [DOCKER.md](DOCKER.md)

### Opcja 2: Lokalne uruchomienie

#### 1. Backend (FastAPI)
```bash
cd python_agent
# try dev autoreload (zalecane):
uvicorn main_fastapi:app --reload --host 0.0.0.0 --port 8080
# lub:
$env:RELOAD='true'; python main_fastapi.py
```
**Server**: http://localhost:8080

#### 2. Frontend (Flutter)
```bash
cd presentation/ui/flutter_voice_ui
flutter run -d web-server --web-port 3000
```
**UI**: http://localhost:3000

#### 3. Vector Database (Qdrant)
```bash
docker run -p 6333:6333 qdrant/qdrant
```
**Vector DB**: http://localhost:6333

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PRESENTATION LAYER                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Flutter UI (Voice + Chat)  â”‚  FastAPI Endpoints           â”‚
â”‚  - Microphone recording     â”‚  - /api/message/stream (SSE) â”‚
â”‚  - Text input               â”‚  - /api/message              â”‚
â”‚  - Chat bubbles             â”‚  - /api/sessions             â”‚
â”‚  - Audio playback           â”‚  - /api/vector/search        â”‚
â”‚  - TTS Queue                â”‚  - /api/knowledge/stats      â”‚
â”‚                              â”‚  - /api/voice/*              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        APPLICATION LAYER                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ConversationAnalysisAgent  â”‚  OrchestrationService        â”‚
â”‚  ChatAgentService           â”‚  ConversationService         â”‚
â”‚  DynamicRAGService          â”‚  PromptService               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚ uses
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          DOMAIN LAYER                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Entities: ChatMessage, RAGChunk, Result                    â”‚
â”‚  Interfaces: ILLMService, IKnowledgeService, Repositories   â”‚
â”‚  Policies: validation, invariants                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚ implemented by
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INFRASTRUCTURE LAYER                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Vector Database (Qdrant)  â”‚  LLM Service                  â”‚
â”‚  - Embedding storage       â”‚  - LM Studio/Ollama           â”‚
â”‚  - Similarity search       â”‚  - Text generation            â”‚
â”‚  - Context retrieval       â”‚  - Response processing        â”‚
â”‚                            â”‚                               â”‚
â”‚  SQLite ChatRepository     â”‚  Text / Audio Services        â”‚
â”‚  - CRUD/Threads/Stats      â”‚  - Cleaning / STT / TTS       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ§  Key Features

### âœ… Ostatnie zmiany (2025-11-11)
- **Docker Support**: PeÅ‚na konteneryzacja backendu i frontendu z Docker Compose
- **Dynamic RAG**: Inteligentne zapytania do bazy wektorowej generowane przez LLM
- **Streaming Responses**: Server-Sent Events (SSE) dla czasu rzeczywistego
- **Voice Chat**: Nagrywanie gÅ‚osu, transkrypcja i synteza mowy
- **TTS Queue**: Kolejkowanie zdaÅ„ dla pÅ‚ynnego odtwarzania audio
- **Debug Panel**: Panel debugowy w Flutterze z 200 ostatnimi logami
- **SzczegÃ³Å‚owe logi**: Kompleksowe logowanie procesu RAG i wyszukiwania
- **Polskie tÅ‚umaczenia**: Wszystkie system prompty i komunikaty po polsku
- **Auto-reload w dev**: `uvicorn main_fastapi:app --reload` lub `$env:RELOAD='true'; python main_fastapi.py`


### ğŸ¤– Conversation Analysis Agent
- **Inteligentna analiza kontekstu** rozmowy
- **Automatyczne decydowanie** o zapytaniach do bazy wektorowej
- **MetamyÅ›lenie refleksyjne** z idiomami matematycznymi
- **Adaptacyjne wyszukiwanie** na podstawie kontekstu

### ğŸ¤ Voice Interface
- **Nagrywanie gÅ‚osu** z mikrofonu
- **Wpisywanie tekstu** jako alternatywa
- **Odtwarzanie odpowiedzi** AI
- **Kontrola audio** (mute/unmute)

### ğŸ’¬ Chat Interface
- **BÄ…belki rozmowy** z avatarem
- **Historia rozmÃ³w** w sesji
- **Automatyczne przewijanie**
- **Centralizowane zarzÄ…dzanie kolorami**

### ğŸ” Vector Database Integration (RAG)
- **Dynamic RAG**: LLM generuje zapytania do bazy wektorowej na podstawie kontekstu rozmowy
- **Inteligentne filtrowanie**: Score threshold (0.50) dla jakoÅ›ci wynikÃ³w
- **Idiomy matematyczne**: Automatyczne pobieranie idiomÃ³w jako system prompt
- **Kontekst w czasie rzeczywistym**: Wyniki RAG dodawane do promptu przed odpowiedziÄ… LLM
- **SzczegÃ³Å‚owe logi**: PeÅ‚ne Å›ledzenie procesu wyszukiwania i filtrowania

## ğŸ”„ Request Flow

```
1. ğŸ‘¤ User Input (Voice/Text)
   â†“
2. ğŸ“± Flutter UI â†’ HTTP POST /api/message/stream (SSE)
   â†“
3. ğŸŒ FastAPI Backend Processing (Streaming):
   â”œâ”€ ğŸ“ Create/Get Session
   â”œâ”€ ğŸ” Get Idioms from Vector DB (System Prompt)
   â”œâ”€ ğŸ’¬ Get Conversation History
   â”œâ”€ ğŸ¤– Dynamic RAG: LLM generuje zapytanie do bazy wektorowej
   â”œâ”€ ğŸ“š Wyszukiwanie w bazie wektorowej (score threshold: 0.50)
   â”œâ”€ ğŸ¯ Build System Prompt z kontekstem RAG
   â”œâ”€ ğŸ­ Process through LLM Service (Streaming)
   â””â”€ ğŸ’¾ Save Conversation to Session
   â†“
4. ğŸ“± Flutter UI â† SSE Stream (chunks + status)
   â”œâ”€ ğŸ“¨ Chunk: Fragment odpowiedzi
   â”œâ”€ ğŸ“š Status: Informacje o RAG
   â””â”€ âœ… Done: ZakoÅ„czenie streamingu
   â†“
5. ğŸ”Š TTS Queue: Automatyczne odtwarzanie zdaÅ„
```

## ğŸ“Š System Components

### Backend Services
- **ConversationAnalysisAgent**: Analiza kontekstu i decydowanie o zapytaniach
- **OrchestrationService**: Koordynacja wszystkich serwisÃ³w
- **ChatAgentService**: ZarzÄ…dzanie agentem i dostÄ™p do wiedzy
- **ConversationService**: ZarzÄ…dzanie sesjami i historiÄ… rozmÃ³w
- **KnowledgeService**: Integracja z bazÄ… wektorowÄ…

### Frontend Components
- **ChatMessage**: Model danych dla wiadomoÅ›ci
- **AppColors**: Centralizowane zarzÄ…dzanie kolorami
- **Voice Recording**: Nagrywanie i przetwarzanie audio
- **Audio Playback**: Odtwarzanie odpowiedzi AI
- **Chat Interface**: Interfejs czatu z bÄ…belkami

### Infrastructure
- **Vector Database (Qdrant)**: Przechowywanie embeddingÃ³w i wyszukiwanie
- **LLM Service**: Generowanie odpowiedzi (LM Studio/Ollama)
- **Text Processing**: Czyszczenie tekstu i obsÅ‚uga Unicode
- **Audio Services**: Speech-to-Text i Text-to-Speech

## ğŸ¯ Reflective Meta-Thinking System

System uÅ¼ywa idiomÃ³w matematycznych jako system prompt dla metamyÅ›lenia refleksyjnego:

```
â¨ # Operator sumy idiomatycznej (Å‚Ä…czenie idiomÃ³w)
Î¦ # Wektor znaczeniowy (meaning vector)
Î¨ # Åšlad idiomu (idiom trace)
Î # Baza semantyczna (semantic basis)
Î£ # Projekcja intencji (intent projection)
Î˜ # Operator metryczny (np. iloczyn skalarny znaczeÅ„)
Î© # PrzestrzeÅ„ funkcyjna idiomu (np. kontekst, intencja, emocja)
... (20 idiomÃ³w)
```

### Context Building Process
1. **Idioms**: 20 wynikÃ³w z bazy wektorowej jako system prompt
2. **History**: 2 poprzednie interakcje user/assistant
3. **Current**: Obecne pytanie uÅ¼ytkownika
4. **Analysis**: Agent analizuje wszystko i decyduje o vector query

## ğŸ“± Flutter Frontend

### Features
- **Voice Recording**: Nagrywanie gÅ‚osu z mikrofonu
- **Text Input**: Wpisywanie tekstu jako alternatywa
- **Chat Interface**: BÄ…belki rozmowy z avatarem i timestampami
- **Audio Playback**: Odtwarzanie odpowiedzi AI
- **Mute Control**: WyÅ‚Ä…czanie/wÅ‚Ä…czanie audio
- **Session Management**: Automatyczne zarzÄ…dzanie sesjami
- **Centralized Colors**: Centralizowane zarzÄ…dzanie kolorami

### UI Components
```dart
class ChatMessage {
  final String text;
  final bool isUser;
  final DateTime timestamp;
  final String? audioUrl;
}

class AppColors {
  static const Color userMessageBg = Color(0xFFB8E6B8);
  static const Color aiMessageBg = Color(0xFFF0F8FF);
  // ... wiÄ™cej kolorÃ³w
}
```

## ğŸŒ FastAPI Backend

### Key Endpoints
- `POST /api/message/stream` - **Streaming endpoint (SSE)** - gÅ‚Ã³wne przetwarzanie wiadomoÅ›ci z RAG
- `POST /api/message` - Synchroniczne przetwarzanie wiadomoÅ›ci
- `POST /api/sessions` - Tworzenie nowej sesji
- `GET /api/sessions/{session_id}` - Pobieranie sesji
- `GET /api/sessions/{session_id}/history` - Historia rozmowy w sesji
- `GET /api/sessions/active` - Lista aktywnych sesji
- `DELETE /api/sessions/{session_id}` - Usuwanie sesji
- `POST /api/vector/search` - Wyszukiwanie w bazie wektorowej
- `GET /api/knowledge/stats` - Statystyki bazy wiedzy
- `GET /api/capabilities` - MoÅ¼liwoÅ›ci serwisÃ³w
- `POST /api/voice/transcribe` - Transkrypcja audio (Speech-to-Text)
- `POST /api/voice/speak` - Synteza mowy (Text-to-Speech)

### Dependency Injection
```python
# Container setup
chat_agent_service = providers.Singleton(ChatAgentService, ...)
conversation_analysis_agent = providers.Singleton(ConversationAnalysisAgent, ...)
orchestration_service = providers.Singleton(OrchestrationService, ...)
```

## ğŸ” Vector Database Integration (RAG)

### Configuration
- **Provider**: Qdrant
- **URL**: http://localhost:6333 (lub `http://host.docker.internal:6333` w Docker)
- **Collections**:
  - `CuratedIdiomsForAI` - **Kolekcja idiomÃ³w** (refleksyjne idiomy matematyczne)
  - `PierwszaKolekcjaOnline` - **Standardowa kolekcja** (ogÃ³lne dane, dynamic RAG)
  - `chat_collection` - Kolekcja czatu (opcjonalna)
- **Embedding Provider**: LM Studio (lub inny z `EMBEDDING_PROVIDER`)
- **Score Threshold**: 0.50 (dla dynamic RAG), 0.75 (dla idiomÃ³w)

### Dynamic RAG Process
1. **LLM Analysis**: LLM analizuje kontekst rozmowy i generuje zapytanie do bazy wektorowej
2. **Vector Search**: Wyszukiwanie w Qdrant z embedding service
3. **Filtering**: Filtrowanie wynikÃ³w wedÅ‚ug score threshold (0.50)
4. **Context Formatting**: Konwersja wynikÃ³w do formatu RAGResult
5. **System Message**: Dodanie kontekstu RAG jako wiadomoÅ›Ä‡ systemowa przed odpowiedziÄ… LLM

### Idioms Search
- **Collection**: `CuratedIdiomsForAI` - dedykowana kolekcja dla idiomÃ³w
- **Hardcoded Query**: "IDIOM_REFLECT REFLECTIVE THINKING CONCEPTS" dla refleksyjnych idiomÃ³w matematycznych
- **TopK**: 20 wynikÃ³w
- **Usage**: Automatyczne dodawanie do system prompt przed kaÅ¼dÄ… odpowiedziÄ… LLM

## ğŸ­ Service Orchestration

### OrchestrationService
Koordynuje wszystkie serwisy i routuje Å¼Ä…dania:
- **Weather Service**: Pogoda dla miast
- **Time Service**: Czas i strefy czasowe
- **City Service**: Informacje o miastach
- **Knowledge Service**: Baza wiedzy i vector search
- **Conversation Service**: ZarzÄ…dzanie rozmowami

## ğŸ“Š Session Management

### ConversationService
- **Session Creation**: Automatyczne tworzenie sesji
- **Message Storage**: Przechowywanie historii rozmÃ³w
- **Context Retrieval**: Pobieranie kontekstu dla analizy
- **Session Cleanup**: Czyszczenie nieaktywnych sesji

## ğŸ”§ Configuration

### Environment Variables
```bash
# LLM Configuration
LLM_PROVIDER=lmstudio
# Dla lokalnego uruchomienia: http://127.0.0.1:8123
# Dla Docker: http://host.docker.internal:8123
LMSTUDIO_LLM_PROXY_URL=http://host.docker.internal:8123
LMSTUDIO_LLM_MODEL_NAME=model:1

# Embedding Configuration
EMBEDDING_PROVIDER=lmstudio
# Dla lokalnego uruchomienia: http://127.0.0.1:8123
# Dla Docker: http://host.docker.internal:8123
LMSTUDIO_PROXY_URL=http://host.docker.internal:8123

# Vector Database
VECTOR_DB_PROVIDER=qdrant
# Dla lokalnego uruchomienia: http://localhost:6333
# Dla Docker: http://qdrant:6333
QDRANT_URL=http://qdrant:6333
LOCAL_SEARCH_INDEX=PierwszaKolekcjaOnline

# Server Configuration
API_HOST=0.0.0.0
API_PORT=8080
FRONTEND_PORT=3000
```

**WiÄ™cej opcji**: Zobacz [env.example](env.example) dla peÅ‚nej listy zmiennych konfiguracyjnych.

### Dependencies
```python
# Core dependencies
fastapi>=0.104.0
uvicorn>=0.24.0
qdrant-client>=1.6.0
dependency-injector>=4.41.0

# Flutter dependencies
flutter: ^3.16.0
http: ^1.1.0
record: ^5.0.4
audioplayers: ^5.2.1
```

## ğŸš€ Deployment

### Backend (FastAPI)
```bash
cd python_agent
python main_fastapi.py
```

### Frontend (Flutter)
```bash
cd presentation/ui/flutter_voice_ui
flutter run -d web-server --web-port 3000
```

### Vector Database (Qdrant)
```bash
docker run -p 6333:6333 qdrant/qdrant
```

## ğŸ“ˆ Performance Metrics

### Response Times
- **Vector Search**: ~200-500ms
- **LLM Processing**: ~1-3s
- **Total Response**: ~2-4s

### Scalability
- **Concurrent Sessions**: 100+
- **Vector Search**: 20 wynikÃ³w na zapytanie
- **Memory Usage**: ~500MB base

## ğŸ› Troubleshooting

### Common Issues
1. **Port Conflicts**: SprawdÅº czy porty 8080, 3000, 6333 sÄ… wolne
2. **Vector DB Connection**: SprawdÅº czy Qdrant dziaÅ‚a
3. **LLM Service**: SprawdÅº czy LM Studio/Ollama dziaÅ‚a
4. **Audio Issues**: SprawdÅº uprawnienia mikrofonu

### Debug Mode
```python
# Enable debug logging
import logging
logging.basicConfig(level=logging.DEBUG)
```

### Debug Tools
Projekt zawiera narzÄ™dzia debugowe w katalogu `tests/`:

```bash
# Analiza struktury danych z bazy wektorowej
python tests/check_chunks.py

# Monitor logÃ³w FastAPI w czasie rzeczywistym
python tests/check_debug_logs.py

# Analiza promptÃ³w wysyÅ‚anych do LLM
python tests/check_llm_input.py

# Test endpointÃ³w API
python tests/test_endpoint.py
```

**WiÄ™cej informacji:** [Debug Tools Documentation](docs/DEBUG_TOOLS.md)

## ğŸ“š Documentation

- **[Project Overview](docs/PROJECT_OVERVIEW.md)** - PrzeglÄ…d projektu i funkcjonalnoÅ›ci
- **[Architecture](docs/ARCHITECTURE.md)** - SzczegÃ³Å‚owa architektura systemu
- **[API Endpoints](docs/API_ENDPOINTS.md)** - Dokumentacja REST API
- **[Flutter Voice UI](docs/FLUTTER_VOICE_UI.md)** - Dokumentacja frontend Flutter
- **[Debug Tools](docs/DEBUG_TOOLS.md)** - NarzÄ™dzia debugowe i analityczne

## ğŸ”® Future Enhancements

### Planned Features
1. **Multi-language Support**: ObsÅ‚uga wielu jÄ™zykÃ³w
2. **Advanced Analytics**: SzczegÃ³Å‚owe analizy rozmÃ³w
3. **Custom Idioms**: UÅ¼ytkownik moÅ¼e dodawaÄ‡ wÅ‚asne idiomy
4. **Voice Cloning**: Klonowanie gÅ‚osu uÅ¼ytkownika
5. **Real-time Collaboration**: WspÃ³Å‚praca w czasie rzeczywistym

### Technical Improvements
1. **Caching**: Cache dla vector search âœ… (czÄ™Å›ciowo - memory cache)
2. **Streaming**: Streaming odpowiedzi âœ… (SSE zaimplementowane)
3. **Batch Processing**: Przetwarzanie wsadowe
4. **Monitoring**: Zaawansowane monitorowanie âœ… (szczegÃ³Å‚owe logi RAG)
5. **Testing**: Kompleksowe testy âœ… (testy jednostkowe i integracyjne)

## ğŸ“ Changelog

### v1.2.0 (2025-11-11)
- âœ… **Docker Support**: PeÅ‚na konteneryzacja backendu i frontendu z Docker Compose
- âœ… **Dynamic RAG**: LLM generuje zapytania do bazy wektorowej na podstawie kontekstu
- âœ… **Streaming Responses**: Server-Sent Events (SSE) dla czasu rzeczywistego
- âœ… **Voice Chat**: Nagrywanie gÅ‚osu, transkrypcja i synteza mowy
- âœ… **TTS Queue**: Kolejkowanie zdaÅ„ dla pÅ‚ynnego odtwarzania audio
- âœ… **Polskie tÅ‚umaczenia**: Wszystkie system prompty i komunikaty po polsku
- âœ… **SzczegÃ³Å‚owe logi**: Kompleksowe logowanie procesu RAG i wyszukiwania
- âœ… **Debug Panel**: Panel debugowy w Flutterze z 200 ostatnimi logami
- âœ… **Nginx Configuration**: Proxy dla frontendu i backendu z timeoutami
- âœ… **Code Cleanup**: UsuniÄ™cie niepotrzebnych print() i DEBUG logÃ³w

### v1.1.0 (2024-10-30)
- âœ… Sklejanie wszystkich system promptÃ³w w JEDEN `SYSTEM`
- âœ… Poprawiona alternacja rÃ³l dla LM Studio
- âœ… Stabilizacja streamingu
- âœ… Auto-reload w dev
- âœ… Globalny `conftest.py` dla testÃ³w

### v1.0.0 (2024-01-01)
- âœ… Initial implementation
- âœ… Flutter voice UI with microphone and chat interface
- âœ… FastAPI backend with Clean Architecture
- âœ… Vector database integration (Qdrant)
- âœ… Conversation Analysis Agent for intelligent context analysis
- âœ… Reflective meta-thinking system with mathematical idioms
- âœ… Session management and conversation history
- âœ… Audio processing (Speech-to-Text, Text-to-Speech)
- âœ… Centralized color management
- âœ… Error handling and fallbacks
- âœ… Health checks and monitoring

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ“ Support

For support and questions:
- Create an issue in the repository
- Check the documentation
- Review the troubleshooting guide

---

**Last Updated**: 2025-11-11  
**Version**: 1.2.0  
**Status**: Production Ready